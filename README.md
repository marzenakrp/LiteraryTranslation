# ğŸ“š Translating World Literature at a Paragraph-Level

[![arxiv](https://img.shields.io/badge/arXiv-2304.03245-b31b1b.svg)](http://arxiv.org/abs/2304.03245)

This is the official repository for evaluation data from "Large language models effectively leverage document-level context for literary translation, but critical errors persist" (check out our [paper](http://arxiv.org/abs/2304.03245) for details).

## UPDATES

ğŸ“š Added prompt demonstrations in the `base_data` folder. The file contains `book` (book title), `source` (source paragraph), `target` (target paragraph), `source_lang` (source language), and `target_lang` (target language).

ğŸ“š Added source data in the `base_data` folder. The file contains `source` (source text used for translation), `target` (target text roughly aligned for reference purpose only; human translation not used in the experiments), `source_lang` (source language), and `target_lang` (target language).

ğŸ“š  Added sentencized data in the `base_data` folder. 

## DATA


This repository contains error annotations on machine translated literary work. The annotations were performed on two translation outputs at a time in [LabelStudio](https://labelstud.io/). You can view the actual interface simply by downloading the data in `./annotations/labelstudio_format_v1` and unploading it to LabelStudio. You will also need to use the code in `labelstudio_interface_code` as the interface. If you just want to download the actual data you should download the `./annotations/default_v1` file. This file contains the pairwise comparisons along with error annotations for each translation and translator's comments. Here is the general structure:

```json
{
  "eval": "fr_ja_1",
  "book": "la_liseuse",
  "author": "paul_fournel",
  "source": "Celui qui est sous ma joue est un manuscrit dâ€™amour : câ€™est lâ€™histoire dâ€™un mec qui rencontre une fille mais il est mariÃ© et elle a un copainâ€¦ Jâ€™en ai lu sept pages et je le connais dÃ©jÃ  par cÅ“ur. Rien ne pourra me surprendre. Depuis des lunes, je ne lis plus, je relis. La mÃªme vieille bouillie dont on fait des Â« nouveautÃ©s Â», des saisons, des rentrÃ©es Â« littÃ©raires Â», des succÃ¨s, des bides, des bides. Du papier quâ€™on recycle, des camions qui partent le matin et qui rentrent le soir, bourrÃ©s de nouveautÃ©s dÃ©jÃ  hors dâ€™Ã¢ge.",
  "target": "ç§ã®é ¬ã®ä¸‹ã«ã‚ã‚‹ã®ã¯ã€æ‹æ„›å°èª¬ã®åŸç¨¿ã€‚ç”·ã¨å¥³ãŒå‡ºä¼šã„ã€ç”·ã¯çµå©šã—ã¦ã„ã¦ã€å¥³ã«ã¯å½¼æ°ãŒã„ã‚‹ã¨ã„ã†ç‰©èªâ€¦â€¦ä¸ƒãƒšãƒ¼ã‚¸èª­ã‚ã°ã€ã‚ã¨ã¯èª­ã¾ãªãã¦ã‚‚ã‚ã‹ã‚‹ã€‚ã¯ã£ã¨é©šã‹ã•ã‚Œã‚‹ã‚ˆã†ãªå ´é¢ã¯å‡ºã¦ã“ãªã„ã ã‚ã†ã€‚ãšã£ã¨å‰ã‹ã‚‰ã€æ–°ãŸã«èª­ã‚€ã¨ã„ã†ã‚ˆã‚Šã¯å†èª­ã€‚ç¹°ã‚Šè¿”ã•ã‚Œã‚‹ã€Œæ–°ä½œã€ã€å·¡ã‚Šãã‚‹å­£ç¯€ã€ç§‹ã®æ–‡èŠ¸è³ãƒ¬ãƒ¼ã‚¹ã€å½“ãŸã£ã¦ã¯å¤–ã‚Œã€ãã—ã¦ã¾ãŸå¤–ã‚Œã‚‹ã€‚ãƒªã‚µã‚¤ã‚¯ãƒ«ã«å‡ºã™ç´™ã®å±±ã€æ™‚ä»£é…ã‚Œã«ãªã£ãŸæ–°ä½œã‚’æº€è¼‰ã—ã€æœã«å‡ºç™ºã—ã¦ã€å¤œã«å¸°ã£ã¦ãã‚‹ä½•å°ã‚‚ã®ãƒˆãƒ©ãƒƒã‚¯ã€‚",
  "source_lang": "fr",
  "target_lang": "ja",
  "text1": "ç§ã®é ¬ã«ã‚ã‚‹ã®ã¯ã€æ„›ã®ãŸã‚ã®æ‰‹ç¨¿ã ã€‚ãã‚Œã¯ã€ç”·ãŒå¥³ã«å‡ºä¼šã†ç‰©èªã ãŒã€å½¼ã¯æ—¢å©šè€…ã§ã€å½¼å¥³ã¯å½¼æ°ãŒã„ã‚‹â€¦â€¦ã¨ã„ã†ç‰©èªã§ã‚ã‚‹ã€‚7ãƒšãƒ¼ã‚¸èª­ã‚“ã ã ã‘ã§ã€ã‚‚ã†è¦šãˆã¦ã—ã¾ã£ãŸã€‚ä½•ã‚‚ç§ã‚’é©šã‹ã›ã‚‹ã“ã¨ã¯ãªã„ã€‚æœˆæ—¥ãŒçµŒã¤ã«ã¤ã‚Œã¦ã€ã‚‚ã†æ–°ã—ã„æœ¬ã‚’èª­ã‚€ã“ã¨ã¯ãªããªã‚Šã€ç¹°ã‚Šè¿”ã—èª­ã‚€ã“ã¨ã«ãªã£ãŸã€‚ãã‚Œã¯ã€æ–°ã—ã„ã‚‚ã®ã‚’ä½œã‚Šå‡ºã™ãŸã‚ã«ä½¿ã‚ã‚Œã‚‹å¤ã„ç²¥ã®ã‚ˆã†ãªã‚‚ã®ã§ã‚ã‚Šã€å­£ç¯€ã‚„æ–°å­¦æœŸã€æ–‡å­¦çš„ãªæˆåŠŸã‚„å¤±æ•—ã€å¤§å¤±æ•—ãªã©ã‚’ä½œã‚Šå‡ºã™ã‚‚ã®ã ã€‚ãƒªã‚µã‚¤ã‚¯ãƒ«ã—ãŸç´™ã€æœå‡ºã‹ã‘ã¦å¤œå¸°ã£ã¦ãã‚‹ãƒˆãƒ©ãƒƒã‚¯ãŒã€ã™ã§ã«å¤è‡­ã„æ–°å•†å“ã§ã„ã£ã±ã„ã ã€‚",
  "text2": "ç§ã®ä¸‹é ¬ã®ä¸‹ã«ã‚ã‚‹ã®ã¯ã€æ„›ã®ç‰©èªã®æ‰‹æ›¸ãåŸç¨¿ã ã€‚ãã‚Œã¯ã€ç”·ãŒå¥³ã«å‡ºä¼šã†ç‰©èªã ãŒã€å½¼ã¯æ—¢ã«çµå©šã—ã¦ã„ã¦ã€å½¼å¥³ã‚‚å½¼æ°ãŒã„ã‚‹â€¦â€¦ã€‚ç§ã¯ãã®ä¸ƒãƒšãƒ¼ã‚¸ã‚’èª­ã‚“ã ãŒã€ã‚‚ã†å¿ƒã‹ã‚‰è¦šãˆã¦ã„ã‚‹ã€‚ä½•ã‚‚é©šã‹ã›ã‚‹ã‚‚ã®ã¯ãªã„ã€‚æœˆæ—¥ãŒçµŒã¤ã†ã¡ã«ã€ç§ã¯ã‚‚ã†æ–°ã—ã„ã‚‚ã®ã‚’èª­ã‚€ã®ã§ã¯ãªãã€å¤ã„ã‚‚ã®ã‚’æ”¹ã‚ã¦èª­ã‚“ã§ã„ã‚‹ã€‚ã€Œæ–°ã—ã„ã‚‚ã®ã€ã¨ã—ã¦ã€å­£ç¯€ã‚„æ–°å­¦æœŸã€æ–‡å­¦çš„ãªã€ŒæˆåŠŸã€ã‚„ã€Œå¤±æ•—ã€ã¨ã—ã¦ã€ä½•åº¦ã‚‚ä½•åº¦ã‚‚å–ã‚Šä¸Šã’ã‚‰ã‚Œã‚‹å¤ã„ã‚‚ã®ã ã€‚æœã«å‡ºã¦ã€å¤œã«å¸°ã£ã¦ãã‚‹ãƒˆãƒ©ãƒƒã‚¯ã¯ã€ã™ã§ã«æ™‚ä»£é…ã‚Œã®ã€Œæ–°ã—ã„ã‚‚ã®ã€ã§ã„ã£ã±ã„ã ã€‚",
  "text1_tag": "sent",
  "text2_tag": "para",
  "difficult_choice": "YES, both translations are close in quality and it was hard to choose between them.",
  "comment": "Both translations are relatively good and easy to read. Both translations translate the \"La mÃªme vieille...\" sentence in a way that it is very difficult to understand - almost word by word especially in T1. Also the æ„›ã®ãŸã‚ã®æ‰‹ç¨¿ in T1 is awkward and later ã¨ã„ã†ç‰©èªã§ã‚ã‚‹ is not necessary and ends in ã§ã‚ã‚‹ which doesn't fit well with other sentences.\nThe ending ã™ã§ã«æ™‚ä»£é…ã‚Œã®ã€Œæ–°ã—ã„ã‚‚ã®ã€ã§ã„ã£ã±ã„ã  in T2 is really good which is why I chose this translation however they are neck and neck and the other one could have be chosen as well.",
  "choice": "text2",
  "added_omitted": "TRANSLATION 2 adds or omits information that significantly change the meaning of the text.",
  "annotations": {
    "text1": [
      {
        "start": 2,
        "end": 6,
        "annotated_text": "é ¬ã«ã‚ã‚‹",
        "labels": ["MISTRANSLATION"]
      },
      {
        "start": 135,
        "end": 168,
        "annotated_text": "ãã‚Œã¯ã€æ–°ã—ã„ã‚‚ã®ã‚’ä½œã‚Šå‡ºã™ãŸã‚ã«ä½¿ã‚ã‚Œã‚‹å¤ã„ç²¥ã®ã‚ˆã†ãªã‚‚ã®ã§ã‚ã‚Š",
        "labels": ["MISTRANSLATION"]
      }
    ],
    "text2": [
      {
        "start": 2,
        "end": 4,
        "annotated_text": "ä¸‹é ¬",
        "labels": ["MISTRANSLATION"]
      }
    ]
  }
}
```

Each entry contains the following information:
- `eval`: *(str)* specifies the type of evaluation; numbers correspond to three different pairwise comparisons described in the paper: (1) SENT vs PARA, (2) PARA_SENT vs PARA, (3) GTr vs PARA;
- `book`: *(str)* title of the book;
- `author`: *(str)* author of the book;
- `source`: *(str)* source passage;
- `target`: *(str)* target passage;
- `source_lang`: *(str)* language code for the source language;
- `target_lang`: *(str)* language code for the target language;
- `text1`: *(str)* translation presented to the annotators as translation 1;
- `text2`: *(str)* translation presented to the annotators as translation 2;
- `text1_tag`: *(str)* specifies the type of translation 1 as one of the following: SENT (sentence-level translation with `davinci-003`), PARA (paragraph-level translation with `davinci-003`), PARA_SENT (sentence-level translation including context with `davinci-003`), GTr (output of `GoogleTranslate`);
- `text2_tag`: *(str)* specifies the type of translation 2 as one of the following: SENT (sentence-level translation with `davinci-003`), PARA (paragraph-level translation with `davinci-003`), PARA_SENT (sentence-level translation including context with `davinci-003`), GTr (output of `GoogleTranslate`);
- `difficult_choice`: *(str)* indicator of whether the translator found both translations of similar quality (multiple-choice);
- `comment`: *(str)* translator's comment about the two translation, all references to translations were unified so that translation 1 is referred to as `T1` while translation 2 is referred to as `T2`. Comments made in Polish or Japanese were translated into English;
- `choice`: *(str)* translator's preferences as to which of the two translations is better (multiple-choice);
- `added_omitted`: *(str)* information on whether any of the texts adds or omits important information (multiple-choice);
- `annotations`: *(dict)* a dictionary containing error annotations for `text1` and `text2`. `start` and `end` indicate indices in the given text, `labels` is a list with all labels assigned to the given span.

Translation errors in `v1` can be one of the following:
- `MISTRANSLATION` - text was mistranslated or translated overly literally;
- `GRAMMAR` - translation contains a grammar error irrespective of the source text;
- `UNTRANSLATED` - text was left in the source language or only transliterated into the target language;
- `INCONSISTENT` - translation uses inconsistent terms to refer to the same entity where the same terms should be used;
- `FORMAT` - translation uses wrong format for the target language (mostly wrong punctuation in Japanese);
- `REGISTER` - translation uses wrong register with respect to the source text (annotated mostly for Japanese were these errors were aparent);
- `REPETITION` - text was repeated in the translation (annotated later for PARA_SENT).

### Disclaimer

The selection of novel fragments curated for this evaluation covers a wide variety of topics. Please be aware that among these, certain subject may delve into sensitive issues including mental health struggles such as depression.

During the final check and translation of comments made in languages other than English a few inconsistencies were spotted. These were corrected in the annotations, however, the numbers in the paper were not updated yet. These corrections do NOT change the overall conclusion, if anything strengthened it, and will be corrected in the paper as soon as possible (version published as WMT contains corrected numbers).

## Citation Information
If you use this dataset, please cite it as follows:
```
@inproceedings{karpinska-iyyer-2023-large,
    title = "Large Language Models Effectively Leverage Document-level Context for Literary Translation, but Critical Errors Persist",
    author = "Karpinska, Marzena  and
      Iyyer, Mohit",
    editor = "Koehn, Philipp  and
      Haddow, Barry  and
      Kocmi, Tom  and
      Monz, Christof",
    booktitle = "Proceedings of the Eighth Conference on Machine Translation",
    month = dec,
    year = "2023",
    address = "Singapore",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.wmt-1.41",
    doi = "10.18653/v1/2023.wmt-1.41",
    pages = "419--451",
    abstract = "Large language models (LLMs) are competitive with the state of the art on a wide range of sentence-level translation datasets. However, their ability to translate paragraphs and documents remains unexplored because evaluation in these settings is costly and difficult. We show through a rigorous human evaluation that asking the GPT-3.5 (text-davinci-003) LLM to translate an entire literary paragraph (e.g., from a novel) at once results in higher-quality translations than standard sentence-by-sentence translation across 18 linguistically-diverse language pairs (e.g., translating into and out of Japanese, Polish, and English). Our evaluation, which took approximately 350 hours of effort for annotation and analysis, is conducted by hiring translators fluent in both the source and target language and asking them to provide both span-level error annotations as well as preference judgments of which system{'}s translations are better. We observe that discourse-level LLM translators commit fewer mistranslations, grammar errors, and stylistic inconsistencies than sentence-level approaches. With that said, critical errors still abound, including occasional content omissions, and a human translator{'}s intervention remains necessary to ensure that the author{'}s voice remains intact. We publicly release our dataset and error annotations to spur future research on the evaluation of document-level literary translation.",
}
```
